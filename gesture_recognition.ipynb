{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc3b141-e061-49e9-9327-c6a859142543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:1] global ../modules/videoio/src/cap_v4l.cpp (893) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf824423b354dbf9cb6c3abb28b095c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'', format='jpeg', height='240', width='320'), Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from action_package import ROBOT\n",
    "import time\n",
    "import threading\n",
    "import ctypes\n",
    "import inspect\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Robot object\n",
    "robot = ROBOT()\n",
    "\n",
    "# Camera configuration\n",
    "cam = cv2.VideoCapture(0, cv2.CAP_V4L2)\n",
    "cam.set(3,320)\n",
    "cam.set(4,240)\n",
    "\n",
    "# Thread stop code\n",
    "def _async_raise(tid, exctype):\n",
    "    tid = ctypes.c_long(tid)\n",
    "    if not inspect.isclass(exctype):\n",
    "        exctype = type(exctype)\n",
    "    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))\n",
    "    if res == 0:\n",
    "        raise ValueError(\"invalid thread id\")\n",
    "    elif res != 1:\n",
    "        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n",
    "        raise SystemError(\"PyThreadState_SetAsyncExc failed\")\n",
    "\n",
    "def stop_thread(thread):\n",
    "    _async_raise(thread.ident, SystemExit)\n",
    "\n",
    "# Convert BGR image to JPEG\n",
    "def bgr8_to_jpeg(value, quality=75):\n",
    "    return bytes(cv2.imencode('.jpg', value)[1])\n",
    "\n",
    "# Initializes the display area\n",
    "FGmaskComp_img = widgets.Image(format='jpeg', width=320, height=240)\n",
    "frame_img = widgets.Image(format='jpeg', width=320, height=240)\n",
    "\n",
    "# overall arrangement\n",
    "dispaly_img = widgets.HBox([FGmaskComp_img, frame_img])\n",
    "display(dispaly_img)\n",
    "\n",
    "# Video display and processing\n",
    "def Video_display():\n",
    "    last_gesture = None  # It is used to record the last detected gesture to avoid repeated operations\n",
    "    last_move_time = time.time()  # Record the last time a robot action was performed\n",
    "    gesture_duration = 0.5  # Set the time interval for gesture detection (seconds)\n",
    "    detection_interval = 3  # Gestures are detected every 3 seconds\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 0)\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Color range (skin tone range, palm color) :\n",
    "        lower_skin = np.array([0, 20, 70], dtype=np.uint8)  # Lower bound for skin color\n",
    "        upper_skin = np.array([20, 255, 255], dtype=np.uint8)  # Upper bound for skin color\n",
    "\n",
    "        # color filtration\n",
    "        mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "        FGmaskComp_img.value = bgr8_to_jpeg(mask)\n",
    "\n",
    "        # contour detection\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = sorted(contours, key=lambda x: cv2.contourArea(x), reverse=True)\n",
    "\n",
    "        gesture = None  # By default, no gesture is detected\n",
    "        # If we detect the palm area\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            (x, y, w, h) = cv2.boundingRect(cnt)\n",
    "            if area >= 500:  # Set a threshold to avoid detecting areas that are too small\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 3)  # Marked palm area\n",
    "                roi = mask[y:y + h, x:x + w]\n",
    "\n",
    "                # Convex Hull and Convex Defects are used to identify fingers\n",
    "                hull = cv2.convexHull(cnt)\n",
    "                \n",
    "                # Ensure that the contour has enough points to calculate convexity defects\n",
    "                if len(hull) >= 3:  # Minimum 3 points for a valid convex hull\n",
    "                    try:\n",
    "                        defects = cv2.convexityDefects(cnt, cv2.convexHull(cnt, returnPoints=False))\n",
    "\n",
    "                        # Finger count\n",
    "                        finger_count = 0\n",
    "                        if defects is not None:\n",
    "                            for i in range(defects.shape[0]):\n",
    "                                s, e, f, d = defects[i, 0]\n",
    "                                start = tuple(cnt[s][0])\n",
    "                                end = tuple(cnt[e][0])\n",
    "                                far = tuple(cnt[f][0])\n",
    "\n",
    "                                # Calculate the Angle and determine if it's the tip of your finger\n",
    "                                a = np.linalg.norm(np.array(start) - np.array(far))\n",
    "                                b = np.linalg.norm(np.array(end) - np.array(far))\n",
    "                                c = np.linalg.norm(np.array(start) - np.array(end))\n",
    "\n",
    "                                angle = np.arccos((a ** 2 + b ** 2 - c ** 2) / (2 * a * b)) * 57\n",
    "\n",
    "                                if angle <= 90:  # Less than 90 degrees is a sharp Angle, indicating fingers\n",
    "                                    finger_count += 1\n",
    "                                    cv2.line(frame, start, far, [0, 255, 0], 2)\n",
    "                                    cv2.line(frame, end, far, [0, 255, 0], 2)\n",
    "                    except cv2.error as e:\n",
    "                        print(f\"Error with convexity defects: {e}\")\n",
    "                        continue  # Skip this contour if there's an error\n",
    "\n",
    "                # Judging gesture\n",
    "                if finger_count == 1:\n",
    "                    gesture = \"Left\"\n",
    "                elif finger_count == 2:\n",
    "                    gesture = \"Right\"\n",
    "                elif finger_count == 3:\n",
    "                    gesture = \"Forward\"\n",
    "                elif finger_count == 4:\n",
    "                    gesture = \"Backward\"\n",
    "                elif finger_count == 5:\n",
    "                    gesture = \"Stop\"\n",
    "                break  # Stop further processing when you find the first palm (assuming a palm)\n",
    "\n",
    "        # If 3 seconds have passed since the last detection, detect the gesture\n",
    "        if time.time() - last_move_time > detection_interval:\n",
    "            if gesture != last_gesture:\n",
    "                if gesture == \"Left\":\n",
    "                    robot.moveLeft(30, 1)\n",
    "                    last_move_time = time.time()  # Update robot movement time\n",
    "                elif gesture == \"Right\":\n",
    "                    robot.moveRight(30, 1)\n",
    "                    last_move_time = time.time()  \n",
    "                elif gesture == \"Forward\":\n",
    "                    robot.t_up(30, 1)\n",
    "                    last_move_time = time.time()  \n",
    "                elif gesture == \"Backward\":\n",
    "                    robot.t_down(30, 1)\n",
    "                    last_move_time = time.time()  \n",
    "                elif gesture == \"Stop\":\n",
    "                    robot.t_stop(0)  # Stop robot motion\n",
    "                    last_move_time = time.time()  # Update stop time\n",
    "\n",
    "            last_gesture = gesture  # Update the last detected gesture\n",
    "\n",
    "        # Check if the robot does not receive a new gesture within the specified time, and stop the robot if the time exceeds\n",
    "        if time.time() - last_move_time > gesture_duration:\n",
    "            robot.t_stop(0)\n",
    "\n",
    "        frame_img.value = bgr8_to_jpeg(frame)  # Show current frame\n",
    "\n",
    "    cam.release()\n",
    "\n",
    "t = threading.Thread(target=Video_display)\n",
    "t.setDaemon(True)\n",
    "t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f8eb9dd-8d10-46b6-9427-54aa7baff404",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid thread id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 32\u001b[0m, in \u001b[0;36mstop_thread\u001b[0;34m(thread)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_thread\u001b[39m(thread):\n\u001b[0;32m---> 32\u001b[0m     \u001b[43m_async_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mident\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;167;43;01mSystemExit\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 26\u001b[0m, in \u001b[0;36m_async_raise\u001b[0;34m(tid, exctype)\u001b[0m\n\u001b[1;32m     24\u001b[0m res \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mpythonapi\u001b[38;5;241m.\u001b[39mPyThreadState_SetAsyncExc(tid, ctypes\u001b[38;5;241m.\u001b[39mpy_object(exctype))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid thread id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     28\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mpythonapi\u001b[38;5;241m.\u001b[39mPyThreadState_SetAsyncExc(tid, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid thread id"
     ]
    }
   ],
   "source": [
    "stop_thread(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4a45f-18a8-4fbf-aa68-fc88577867e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convex Hull and Convex Defects are used to identify fingers\n",
    "hull = cv2.convexHull(cnt)\n",
    "# Ensure that the contour has enough points to calculate convexity defects\n",
    "if len(hull) >= 3:  # Minimum 3 points for a valid convex hull\n",
    "    try:\n",
    "        defects = cv2.convexityDefects(cnt, cv2.convexHull(cnt, returnPoints=False))\n",
    "        # Finger count\n",
    "        finger_count = 0\n",
    "        if defects is not None:\n",
    "            for i in range(defects.shape[0]):\n",
    "                s, e, f, d = defects[i, 0]\n",
    "                start = tuple(cnt[s][0])\n",
    "                end = tuple(cnt[e][0])\n",
    "                far = tuple(cnt[f][0])\n",
    "                # Calculate the Angle and determine if it's the tip of your finger\n",
    "                a = np.linalg.norm(np.array(start) - np.array(far))\n",
    "                b = np.linalg.norm(np.array(end) - np.array(far))\n",
    "                c = np.linalg.norm(np.array(start) - np.array(end))\n",
    "                angle = np.arccos((a ** 2 + b ** 2 - c ** 2) / (2 * a * b)) * 57\n",
    "                if angle <= 90:  # Less than 90 degrees is a sharp Angle, indicating fingers\n",
    "                    finger_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
